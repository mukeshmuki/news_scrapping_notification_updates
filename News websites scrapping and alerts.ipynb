{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60730d1c-b246-41d9-a3ad-b6b08d58d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########careers360 ############\n",
    "\n",
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "format_str = '%d-%m-%Y' # The format\n",
    "\n",
    "path = '/Users/mukeshd/Desktop/cswb_scrap/'\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "# Create an URL object\n",
    "s = ['https://news.careers360.com/latest?page=1',\n",
    "     'https://news.careers360.com/latest?page=2'\n",
    "    ]\n",
    "for url in s:\n",
    "    # Create object page\n",
    "    page = requests.get(url)\n",
    "    # parser-lxml = Change html to Python friendly format\n",
    "    # Obtain page's information\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    for x in soup.find_all(\"div\", {\"class\": \"heading4\"}):\n",
    "      title.append(x.get_text(strip=True))\n",
    "\n",
    "    head = 'https://news.careers360.com'\n",
    "    for x in soup.find_all(\"div\", {\"class\": \"heading4\"}):\n",
    "        for a in x.find_all('a', href=True):\n",
    "\n",
    "            links.append(head + a['href'])\n",
    "            #print(\"Found the URL:\", a['href'])\n",
    "\n",
    "    for x in soup.find_all(\"div\", {\"class\": \"arti-Bottom\"}):\n",
    "        datee.append(x.get_text(strip=True).split(\"|\")[1])\n",
    "\n",
    "ddf_1 = pd.DataFrame(zip(title,links, datee ),columns = ['title', 'links', 'date'])\n",
    "#ddf.to_excel(path + \"ddf.xlsx\")\n",
    "\n",
    "\n",
    "#######https://scholarshiparena.in/#####\n",
    "\n",
    "url = 'https://scholarshiparena.in/'\n",
    "page = requests.get(url)\n",
    "\n",
    "    # parser-lxml = Change html to Python friendly format\n",
    "    # Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "title = [x.get_text(strip=True) for x in soup.find_all(\"h2\", {\"class\": \"entry-title\"})]\n",
    "datee = [x.get_text(strip=True) for x in soup.find_all(\"time\", {\"class\": \"entry-date published\"})]\n",
    "links = []\n",
    "for x in soup.find_all(\"h2\", {\"class\": \"entry-title\"}):\n",
    "    for a in x.find_all('a', href=True):\n",
    "        #print(a['href'])\n",
    "        links.append(a['href'])\n",
    "        \n",
    "ddf_2 = pd.DataFrame(zip(title,links, datee ),columns = ['title', 'links', 'date'])        \n",
    "\n",
    "\n",
    "#####NEET-UG ########\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "# Create an URL object\n",
    "url = 'https://nta.ac.in/NoticeBoardArchive'\n",
    "# Create object page\n",
    "page = requests.get(url)\n",
    "\n",
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "# Obtain information from tag <table>\n",
    "table1 = soup.find(\"table\", id = 'tbl')\n",
    "\n",
    "# Obtain every title of columns with tag <th>\n",
    "headers = []\n",
    "for i in table1.find_all('th'):\n",
    " title = i.text\n",
    " headers.append(title)\n",
    "\n",
    "# Create a dataframe\n",
    "mydata = pd.DataFrame(columns = headers)\n",
    "\n",
    "# Create a for loop to fill mydata\n",
    "for j in table1.find_all('tr')[1:]:\n",
    " row_data = j.find_all('td')\n",
    " row = [i.text for i in row_data]\n",
    " length = len(mydata)\n",
    " mydata.loc[length] = row\n",
    "\n",
    "ddf_3 = mydata[:10]\n",
    "\n",
    "\n",
    "#####'https://www.upsc.gov.in/whats-new'\n",
    "\n",
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "format_str = '%d-%m-%Y' # The format\n",
    "\n",
    "#path = '/Users/mukeshd/Desktop/cswb_scrap/'\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "# Create an URL object\n",
    "url = 'https://www.upsc.gov.in/whats-new'\n",
    "    \n",
    "# Create object page\n",
    "page = requests.get(url, verify=False)\n",
    "\n",
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "head = 'https://www.upsc.gov.in/'\n",
    "for x in soup.find_all(\"div\", {\"class\": \"view-header\"}):\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(head + a['href'])\n",
    "        #title.append(a.get_text(strip=True).split(\" \")[0])\n",
    "        title.append(a.get_text(strip=True))\n",
    "\n",
    "for x in soup.find_all(\"div\", {\"class\": \"view-content\"}):\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(head + a['href'])\n",
    "        #title.append(a.get_text(strip=True).split(\" \")[0])\n",
    "        title.append(a.get_text(strip=True))\n",
    "\n",
    "ddf_4 = pd.DataFrame(zip(title,links ),columns = ['title', 'links'])                \n",
    "        #print(\"Found the URL:\", a['href'])\n",
    "\n",
    "#for x in soup.find_all(\"div\", {\"class\": \"arti-Bottom\"}):\n",
    "#    datee.append(x.get_text(strip=True).split(\"|\")[1])\n",
    "#ddf_1 = pd.DataFrame(zip(title,links, datee ),columns = ['title', 'links', 'date'])\n",
    "#ddf.to_excel(path + \"ddf.xlsx\")\n",
    "\n",
    "ddf_4 = ddf_4[:15]\n",
    "\n",
    "\n",
    "#####'https://bihar-cetbed-lnmu.in/'\n",
    "# Create an URL object\n",
    "url = 'https://bihar-cetbed-lnmu.in/'\n",
    "    \n",
    "# Create object page\n",
    "page = requests.get(url, verify=False)\n",
    "\n",
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "\n",
    "head = 'https://bihar-cetbed-lnmu.in/'\n",
    "for x in soup.find_all(\"div\", {\"class\": \"wp-block-group__inner-container\"}):\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        #links.append(head + a['href'])\n",
    "        links.append(a['href'])\n",
    "        \n",
    "        \n",
    "        title.append(a.get_text(strip=True))\n",
    "\n",
    "ddf_5 = pd.DataFrame(zip(title,links ),columns = ['title', 'links'])                \n",
    "ddf_5 = ddf_5[:15]        \n",
    "\n",
    "\n",
    "\n",
    "########Shiksha ############\n",
    "url = 'https://www.shiksha.com/news'\n",
    "    \n",
    "# Create object page\n",
    "page = requests.get(url, verify=False)\n",
    "\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "\n",
    "head = 'https://www.shiksha.com'\n",
    "\n",
    "\n",
    "for x in soup.find_all(\"h3\", {\"class\": \"articleTitle\"}):\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(head + a['href'])\n",
    "        #title.append(a.get_text(strip=True).split(\" \")[0])\n",
    "        title.append(a.get_text(strip=True))\n",
    "                        \n",
    "date = [x.get_text(strip=True) for x in soup.find_all(\"strong\", {\"class\": \"articelUpdatedDate\"})]\n",
    "ddf_6 = pd.DataFrame(zip(title,links, date ),columns = ['title', 'links', 'date'])                \n",
    "\n",
    "\n",
    "##########NDTV###############\n",
    "url = 'https://www.ndtv.com/education/exams-news'\n",
    "    \n",
    "# Create object page\n",
    "page = requests.get(url, verify=False)\n",
    "\n",
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "#title = []\n",
    "#links =[]\n",
    "#datee = []\n",
    "\n",
    "head = 'https://www.ndtv.com'\n",
    "\"\"\"\n",
    "for x in soup.find_all(\"div\", {\"class\": \"detail\"}):\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(head + a['href'])\n",
    "        title.append(a.get_text(strip=True))\n",
    "\n",
    "\"\"\"        \n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "\n",
    "for x in soup.find_all(\"h3\"):\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(head + a['href'])\n",
    "        #title.append(a.get_text(strip=True).split(\" \")[0])\n",
    "        title.append(a.get_text(strip=True))\n",
    "                        \n",
    "date = [x.get_text(strip=True) for x in soup.find_all(\"div\", {\"class\": \"author\"})]\n",
    "\n",
    "dte = []\n",
    "for x in date:\n",
    "    x = x.split(\"|\")[1]\n",
    "    x = x.replace(\"\\n\", \"\")\n",
    "    #x = x.replace(\" \",\"\")\n",
    "    x = x.replace(\"Updated:\",\"\")\n",
    "    \n",
    "    dte.append(x)\n",
    "    \n",
    "        \n",
    "ddf_7 = pd.DataFrame(zip(title,links, dte ),columns = ['title', 'links', 'date'])                \n",
    "ddf_7 = ddf_7\n",
    "\n",
    "\n",
    "##########NDTV###############\n",
    "url = 'https://www.timesnownews.com/education'\n",
    "    \n",
    "# Create object page\n",
    "page = requests.get(url, verify=False)\n",
    "\n",
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "\n",
    "head = 'https://www.timesnownews.com'\n",
    "\n",
    "\n",
    "for x in soup.find_all(\"div\", {\"class\": \"ArticleList-tnn__h-list-1W5s9\"}):\n",
    "\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(head + a['href'])\n",
    "        #title.append(a.get_text(strip=True).split(\" \")[0])\n",
    "        title.append(a.get_text(strip=True))\n",
    "                        \n",
    "#date = [x.get_text(strip=True) for x in soup.find_all(\"div\", {\"class\": \"author\"})]\n",
    "\n",
    "\n",
    "ddf_8 = pd.DataFrame(zip(title,links),columns = ['title', 'links'])                \n",
    "ddf_8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########LIVE MINT###############\n",
    "url = 'https://www.livemint.com/education/news/'\n",
    "    \n",
    "# Create object page\n",
    "page = requests.get(url, verify=False)\n",
    "\n",
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "\n",
    "head = 'https://www.livemint.com/education/news/'\n",
    "\n",
    "\n",
    "for x in soup.find_all(\"h2\", {\"class\": \"headline\"}):\n",
    "\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(a['href'])\n",
    "        #title.append(a.get_text(strip=True).split(\" \")[0])\n",
    "        title.append(a.get_text(strip=True))\n",
    "                        \n",
    "#date = [x.get_text(strip=True) for x in soup.find_all(\"div\", {\"class\": \"author\"})]\n",
    "\n",
    "\n",
    "ddf_9 = pd.DataFrame(zip(title,links),columns = ['title', 'links'])                \n",
    "ddf_9\n",
    "\n",
    "\n",
    "##########HINDUSTAN TIMES###############\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "\n",
    "def extract_source(url):\n",
    "    agent = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "    source=requests.get(url, headers=agent).text\n",
    "    return source\n",
    "\n",
    "def extract_data(source):\n",
    "    soup=bs4.BeautifulSoup(source, 'lxml')\n",
    "    names=soup.findAll('title')\n",
    "    for i in names:\n",
    "        print(i)\n",
    "    head = 'https://www.hindustantimes.com'\n",
    "    for x in soup.find_all(\"h3\", {\"class\": \"hdg3\"}):\n",
    "        for a in x.find_all('a', href=True):\n",
    "            links.append(head + a['href'])\n",
    "            title.append(a.get_text(strip=True))\n",
    "\n",
    "extract_data(extract_source('https://www.hindustantimes.com/education'))\n",
    "\n",
    "\n",
    "ddf_10 = pd.DataFrame(zip(title,links),columns = ['title', 'links'])                \n",
    "ddf_10\n",
    "\n",
    "\n",
    "##########INDIAN EXPRESS###############\n",
    "url = 'https://indianexpress.com/section/education/'\n",
    "    \n",
    "# Create object page\n",
    "page = requests.get(url, verify=False)\n",
    "\n",
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "\n",
    "title = []\n",
    "links =[]\n",
    "datee = []\n",
    "\n",
    "head = 'https://indianexpress.com/article/education'\n",
    "\n",
    "\n",
    "for x in soup.find_all(\"h2\", {\"class\": \"title\"}):\n",
    "\n",
    "    for a in x.find_all('a', href=True):\n",
    "\n",
    "        links.append(a['href'])\n",
    "        #title.append(a.get_text(strip=True).split(\" \")[0])\n",
    "        title.append(a.get_text(strip=True))\n",
    "                        \n",
    "#date = [x.get_text(strip=True) for x in soup.find_all(\"div\", {\"class\": \"author\"})]\n",
    "\n",
    "\n",
    "ddf_11 = pd.DataFrame(zip(title,links),columns = ['title', 'links'])                \n",
    "ddf_11\n",
    "\n",
    "\n",
    "########Writing to google sheet #########\n",
    "#from __future__ import print_function\n",
    "\n",
    "import googleapiclient.discovery as discovery\n",
    "from httplib2 import Http\n",
    "from oauth2client import client\n",
    "from oauth2client import file\n",
    "from oauth2client import tools\n",
    "\n",
    "import pygsheets\n",
    "import pandas as pd\n",
    "#authorization\n",
    "path = '/Users/mukeshd/Downloads/docs/'\n",
    "gc = pygsheets.authorize(service_file= path + 'credentials2.json')\n",
    "\n",
    "\n",
    "#open the google spreadsheet (where 'PY to Gsheet Test' is the name of my sheet)\n",
    "sh = gc.open('Notification Updates')\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[1]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_1,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[2]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_2,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[3]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_3,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[4]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_4,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[5]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_5,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[6]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_6,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[7]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_7,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[8]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_8,(1,1))\n",
    "\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[9]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_9,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[10]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_10,(1,1))\n",
    "\n",
    "#select the first sheet \n",
    "wks = sh[11]\n",
    "\n",
    "#update the first sheet with df, starting at cell B2. \n",
    "wks.set_dataframe(ddf_11,(1,1))\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "\n",
    "message = \"Notification code is run\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = \""\n",
    "    #message = (\"A Sample Message\")\n",
    "    title = (f\"Live Trends\")\n",
    "    slack_data = {\n",
    "        \"username\": \"Notification_updates\",\n",
    "        \"icon_emoji\": \":satellite:\",\n",
    "        #\"channel\" : \"#somerandomcahnnel\",\n",
    "        \"attachments\": [\n",
    "            {\n",
    "                \"color\": \"#9733EE\",\n",
    "                \"fields\": [\n",
    "                    {\n",
    "                        \"title\": title,\n",
    "                        \"value\": message,\n",
    "                        \"short\": \"false\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    byte_length = str(sys.getsizeof(slack_data))\n",
    "    headers = {'Content-Type': \"application/json\", 'Content-Length': byte_length}\n",
    "    response = requests.post(url, data=json.dumps(slack_data), headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
